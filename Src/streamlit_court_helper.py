
import streamlit as st
import easyocr
import json
import bs4
import requests
from transformers import (
    TokenClassificationPipeline,
    AutoModelForTokenClassification,
    AutoTokenizer,
)
from transformers.pipelines import AggregationStrategy
import numpy as np
import PyPDF2


# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
page_title="–ó–¥–µ—Å—å –±—É–¥–µ—Ç –∫—Ä–∞—Å–∏–≤–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ",
page_icon="üßä")

# –ò–∑–º–µ–Ω–µ–Ω–∏–µ –¥–∏–∑–∞–π–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
page_element="""
<style>
[data-testid="stAppViewContainer"]{
  background-image: url("https://i.artfile.ru/1920x1080_927461_[www.ArtFile.ru].jpg");
  background-size: cover;
}
[data-testid="baseButton-secondary"]{
background-color: rgb(176 96 255 / 70%);
}
[role="tablist"]{
background-color: rgb(210 164 255 / 90%);
border-radius: 10px;
}
[data-testid="stHeader"]{
  background-color: rgba(0,0,0,0);
}
.st-b1{
  margin: auto;
}
[data-testid="block-container"]
 { background-color: rgba(255, 255, 255, 0.4)
}
.st-cc {
    margin: auto;
}
</style>
"""

st.markdown(page_element, unsafe_allow_html=True)

# –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.header('–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É–µ–º! –ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.')

# –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –≤–∫–ª–∞–¥–æ–∫ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞
tabs = st.tabs(["–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ", "–ü–æ–∏—Å–∫", "–≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä"])

# –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –≤–∫–ª–∞–¥–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
with tabs[0]:
    @st.cache_resource()
    def easyocr_recognition(img):
        return easyocr.Reader(["ru", "en", "uk"]).readtext(img, detail=0, paragraph=True, text_threshold=0.8)

    # —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é easyocr, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: –æ—Ç–∫–ª—é—á–µ–Ω–∞ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã–≤–æ–¥–∞,
    # –≤–∫–ª—é—á–µ–Ω—ã –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞

    # —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª

    def load_image():
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–æ—Ä–º—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        # –§–æ—Ä–º–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏ Streamlit
        uploaded_file = st.file_uploader(
            label='–í—ã–±–µ—Ä–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, —Å–æ–¥–µ—Ä–∂–∞—â–µ–µ —Ç–µ–∫—Å—Ç, –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è.\n –§–æ—Ä–º–∞—Ç –∑–∞–≥—Ä—É–∂–∞–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞ .jpg .png')
        if uploaded_file is not None:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            img = uploaded_file.getvalue()
            # –ü–æ–∫–∞–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ Web-—Å—Ç—Ä–∞–Ω–∏—Ü–µ —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏ Streamlit
            st.image(img)
            return img
        else:
            return st.write('–í—ã –Ω–∏—á–µ–≥–æ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏!')

    # –í—ã–≤–æ–¥–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏ Streamlit
    st.header('–≠—Ç–æ –ø—Ä–æ—Å—Ç–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö —Ç–µ–∫—Å—Ç')

    # –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–æ—Ä–º—ã –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

    img = load_image()

    result = st.button('–†–∞—Å–ø–æ–∑–Ω–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ —Ñ–∞–π–ª–∞')
    if result:
        try:
            text_result = easyocr_recognition(img)
            st.write(text_result)
        except:
            st.write('–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ñ–∞–π–ª –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏–ª–∏ –Ω–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞!')

# –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –≤–∫–ª–∞–¥–∫–∏ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
with tabs[1]:
    st.header('–≠—Ç–æ –ø—Ä–æ—Å—Ç–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º')
# —Ç–µ–ª–æ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    class Article:

        def __init__(self, title='', link='', authors=''):
            self.__title = title
            self.__link = link
            self.__authors = authors
            self.__year = 0
            self.__rsci = False
            self.__vak = False
            self.__scopus = False

        def check_filter(self, filter):
            if filter == 22 and not self.__rsci or filter == 8 and not self.__vak or filter == 2 and not self.__scopus:
                return False
            return True

        @property
        def title(self):
            return self.__title

        @title.setter
        def title(self, value):
            self.__title = value

        @property
        def link(self):
            return self.__link

        @link.setter
        def link(self, value):
            self.__link = value

        @property
        def authors(self):
            return self.__authors

        @authors.setter
        def authors(self, value):
            self.__authors = value

        @property
        def year(self):
            return self.__year

        @year.setter
        def year(self, value):
            self.__year = value

        @property
        def rsci(self):
            return self.__rsci

        @rsci.setter
        def rsci(self, value):
            self.__rsci = value

        @property
        def vak(self):
            return self.__vak

        @vak.setter
        def vak(self, value):
            self.__vak = value

        @property
        def scopus(self):
            return self.__scopus

        @scopus.setter
        def scopus(self, value):
            self.__scopus = value

    TERM_LINK = '"link":'
    TERM_FOUND = '"found":'

    API = 'https://cyberleninka.ru/api/search'
    URL = 'https://cyberleninka.ru'

    REQUEST_BODY = {
        'mode': 'articles',
    '   size': 10,
    }

    ARTICLES_PER_PAGE = 10


    class Searcher:

        def __try_parse_article(self, link, article):
            response_page = requests.get(link)
            bs_page = bs4.BeautifulSoup(response_page.text, 'html.parser')
            try:
                article.title = bs_page.i.text
            except:
                print('done')

            authors = bs_page.find('h2', {'class': 'right-title'}).span.text
            article.authors = authors[authors.find('‚Äî') + 2:]

            labels = bs_page.find('div', {'class': 'labels'})
            article.year = int(labels.time.text)

            rsci = bs_page.find('div', {'class': 'label rsci'})
            if rsci:
                article.rsci = True

            vak = bs_page.find('div', {'class': 'label vak'})
            if vak:
                article.vak = True

            scopus = bs_page.find('div', {'class': 'label scopus'})
            if scopus:
                article.scopus = True

        def parse_article_page(self, link):
            article = Article(link=link)

            try:
                self.__try_parse_article(link, article)

            except requests.HTTPError or AttributeError as e:
                print(e)

            return article

        def __try_parse_request(self, body, filters, results, articles_per_page=ARTICLES_PER_PAGE):
            response_json = requests.post(API, data=json.dumps(body)).text

            for article_num in range(articles_per_page):
                response_json = response_json[response_json.find(TERM_LINK) + len(TERM_LINK) + 1:]
                article_link = response_json[:response_json.find('"')]
                article = self.parse_article_page(URL + article_link)
                if filters is None or article.check_filter(filters[0]):
                    results.append(article)
                response_json = response_json[response_json.find('}'):]

        def search_articles(self, keywords, max_page, filters=None):
            results = []
            found = 0
            body = {
                'mode': 'articles',
                'size': 10,
                'q': keywords}

            if filters:
                body = body | {'catalogs': filters}

            body['from'] = 0

            try:
                response_json = requests.post(API, data=json.dumps(body)).text
                response_found = response_json[response_json.find(TERM_FOUND) + len(TERM_FOUND):]
                #print(response_found)
                #print(response_json)
                resp_data = json.loads(response_json)
                relevant = resp_data["articles"]
                for i in relevant:

                    def special_char_fix(string):
                        string = list(string)
                        for pl, char in enumerate(string):
                            if char == '\\':
                                val = ''.join([string[pl + k + 2] for k in range(4)])
                                for k in range(5):
                                    string.pop(pl)
                                string[pl] = str(chr(int(val, 16)))
                        return ''.join(string)


                    st.write("–ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏:" + ' ' + (special_char_fix(i["name"])) + ' ' + '–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è:' + ' ' + i["annotation"] + ' ' + i["link"], end='\n')


                found = int(response_found[:response_found.find(',')])

            except requests.HTTPError or AttributeError as e:
                print(e)

            for page in range(max_page):
                try:
                    if found >= ARTICLES_PER_PAGE:
                        body['from'] += 10 * page
                        self.__try_parse_request(body, filters, results)
                        found -= ARTICLES_PER_PAGE
                    else:
                        body['from'] += 10 * (page - 1)
                        self.__try_parse_request(body, filters, results, found)
                        return results

                except requests.HTTPError as e:
                    print(e)
                    return []

            return results





    keywords = st.text_input(('–í–≤–µ–¥–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞.\n').lower())
    result1 = st.button('–í—ã–≤–µ—Å—Ç–∏ —Å–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π')
    if result1:
        try:
            pr = Searcher()
            pr.search_articles(keywords, 10)
        except:
            st.write('–≤—ã –Ω–µ –Ω–∞—á–∞–ª–∏ –ø–æ–∏—Å–∫!')

with tabs[2]:
    st.header('–≠—Ç–æ –ø—Ä–æ—Å—Ç–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞')
    # –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã PDF –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
    from pdfminer.high_level import extract_pages, extract_text
    from pdfminer.layout import LTTextContainer, LTChar, LTRect, LTFigure
    # –î–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ç–∞–±–ª–∏—Ü –≤ PDF
    import pdfplumber


    # –°–æ–∑–¥–∞–µ–º –∫–ª–∞—Å—Å, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∏–π –ø–∞–π–ø–ª–∞–π–Ω –≤—ã–¥–µ–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    class KeyphraseExtractionPipeline(TokenClassificationPipeline):
        def __init__(self, model, *args, **kwargs):
            super().__init__(
                model=AutoModelForTokenClassification.from_pretrained(model),
                tokenizer=AutoTokenizer.from_pretrained(model),
                *args,
                **kwargs
            )

        def postprocess(self, all_outputs):
            results = super().postprocess(
                all_outputs=all_outputs,
                aggregation_strategy=AggregationStrategy.SIMPLE,
            )
            return np.unique([result.get("word").strip() for result in results])

    def filereader(file_path):
        with open(file_path, "r", encoding="utf-8") as file:
            draft = file.read()
        return draft


    def text_extraction(element):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –≤–ª–æ–∂–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
        line_text = element.get_text()

        # –ù–∞—Ö–æ–¥–∏–º —Ñ–æ—Ä–º–∞—Ç—ã —Ç–µ–∫—Å—Ç–∞
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —Å–æ –≤—Å–µ–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏, –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–º–∏—Å—è –≤ —Å—Ç—Ä–æ–∫–µ —Ç–µ–∫—Å—Ç–∞
        line_formats = []
        for text_line in element:
            if isinstance(text_line, LTTextContainer):
                # –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –æ–±—Ö–æ–¥–∏–º –∫–∞–∂–¥—ã–π —Å–∏–º–≤–æ–ª –≤ —Å—Ç—Ä–æ–∫–µ —Ç–µ–∫—Å—Ç–∞
                for character in text_line:
                    if isinstance(character, LTChar):
                        # –î–æ–±–∞–≤–ª—è–µ–º –∫ —Å–∏–º–≤–æ–ª—É –Ω–∞–∑–≤–∞–Ω–∏–µ —à—Ä–∏—Ñ—Ç–∞
                        line_formats.append(character.fontname)
                        # –î–æ–±–∞–≤–ª—è–µ–º –∫ —Å–∏–º–≤–æ–ª—É —Ä–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞
                        line_formats.append(character.size)
        # –ù–∞—Ö–æ–¥–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –∏ –Ω–∞–∑–≤–∞–Ω–∏—è —à—Ä–∏—Ñ—Ç–æ–≤ –≤ —Å—Ç—Ä–æ–∫–µ
        format_per_line = list(set(line_formats))

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ—Ä—Ç–µ–∂ —Å —Ç–µ–∫—Å—Ç–æ–º –≤ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–µ –≤–º–µ—Å—Ç–µ —Å –µ–≥–æ —Ñ–æ—Ä–º–∞—Ç–æ–º
        return (line_text, format_per_line)

    def extract_table(pdf_path, page_num, table_num):
        # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª pdf
        pdf = pdfplumber.open(pdf_path)
        # –ù–∞—Ö–æ–¥–∏–º –∏—Å—Å–ª–µ–¥—É–µ–º—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        table_page = pdf.pages[page_num]
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é —Ç–∞–±–ª–∏—Ü—É
        table = table_page.extract_tables()[table_num]
        return table

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–∞–±–ª–∏—Ü—É –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Ñ–æ—Ä–º–∞—Ç
    def table_converter(table):
        table_string = ''
        # –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –æ–±—Ö–æ–¥–∏–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–æ–∫—É –≤ —Ç–∞–±–ª–∏—Ü–µ
        for row_num in range(len(table)):
            row = table[row_num]
            # –£–¥–∞–ª—è–µ–º —Ä–∞–∑—Ä—ã–≤ —Å—Ç—Ä–æ–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –ø–µ—Ä–µ–Ω–æ—Å–æ–º
            cleaned_row = [item.replace('\n', ' ') if item is not None and '\n' in item else 'None' if item is None else item for item in row]
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–∞–±–ª–∏—Ü—É –≤ —Å—Ç—Ä–æ–∫—É
            table_string+=('|'+'|'.join(cleaned_row)+'|'+'\n')
        # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑—Ä—ã–≤ —Å—Ç—Ä–æ–∫–∏
        table_string = table_string[:-1]
        return table_string

    def crop_image(element, pageObj):
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è –≤—ã—Ä–µ–∑–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ PDF
        [image_left, image_top, image_right, image_bottom] = [element.x0,element.y0,element.x1,element.y1]
        # –û–±—Ä–µ–∑–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º (left, bottom, right, top)
        pageObj.mediabox.lower_left = (image_left, image_bottom)
        pageObj.mediabox.upper_right = (image_right, image_top)
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–µ–∑–∞–Ω–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ –Ω–æ–≤—ã–π PDF
        cropped_pdf_writer = PyPDF2.PdfWriter()
        cropped_pdf_writer.add_page(pageObj)
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π PDF –≤ –Ω–æ–≤—ã–π —Ñ–∞–π–ª
        with open('cropped_image.pdf', 'wb') as cropped_pdf_file:
            cropped_pdf_writer.write(cropped_pdf_file)

    # –°–æ–∑–¥–∞—ë–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Å—á–∏—Ç—ã–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    def image_to_text(image_path):
        # –°—á–∏—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        img = Image.open(image_path)
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        text = pytesseract.image_to_string(img)
        return text



    model_name = "ml6team/keyphrase-extraction-kbir-inspec"
    extractor = KeyphraseExtractionPipeline(model=model_name)


    file_path = st.file_uploader(
                label='–í—ã–±–µ—Ä–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, —Å–æ–¥–µ—Ä–∂–∞—â–µ–µ —Ç–µ–∫—Å—Ç, –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è.\n –§–æ—Ä–º–∞—Ç –∑–∞–≥—Ä—É–∂–∞–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞ .txt, .pdf')

    indent = '.pdf'

    pdf_path = file_path

    # —Å–æ–∑–¥–∞—ë–º –æ–±—ä–µ–∫—Ç —Ñ–∞–π–ª–∞ PDF
    pdfFileObj = open(pdf_path, 'rb')
    # —Å–æ–∑–¥–∞—ë–º –æ–±—ä–µ–∫—Ç —Å—á–∏—Ç—ã–≤–∞—Ç–µ–ª—è PDF
    pdfReaded = PyPDF2.PdfReader(pdfFileObj)

    # –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    text_per_page = {}
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ PDF
    for pagenum, page in enumerate(extract_pages(pdf_path)):

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        pageObj = pdfReaded.pages[pagenum]
        page_text = []
        line_format = []
        text_from_images = []
        text_from_tables = []
        page_content = []
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü
        table_num = 0
        first_element = True
        table_extraction_flag = False
        # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª pdf
        pdf = pdfplumber.open(pdf_path)
        # –ù–∞—Ö–æ–¥–∏–º –∏—Å—Å–ª–µ–¥—É–µ–º—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        page_tables = pdf.pages[pagenum]
        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∞–±–ª–∏—Ü –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        tables = page_tables.find_tables()

        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        page_elements = [(element.y1, element) for element in page._objs]
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ –ø–æ—Ä—è–¥–∫—É –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        page_elements.sort(key=lambda a: a[0], reverse=True)

        # –ù–∞—Ö–æ–¥–∏–º —ç–ª–µ–º–µ–Ω—Ç—ã, —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É
        for i, component in enumerate(page_elements):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–æ–∂–µ–Ω–∏–µ –≤–µ—Ä—Ö–Ω–µ–≥–æ –∫—Ä–∞—è —ç–ª–µ–º–µ–Ω—Ç–∞ –≤ PDF
            pos = component[0]
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            element = component[1]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç–ª–µ–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–º
            if isinstance(element, LTTextContainer):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –≤ —Ç–∞–±–ª–∏—Ü–µ
                if table_extraction_flag == False:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
                    (line_text, format_per_line) = text_extraction(element)
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ –∫ —Ç–µ–∫—Å—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_text.append(line_text)
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏, —Å–æ–¥–µ—Ä–∂–∞—â–µ–π —Ç–µ–∫—Å—Ç
                    line_format.append(format_per_line)
                    page_content.append(line_text)
                else:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ–∫—Å—Ç, –Ω–∞—Ö–æ–¥—è—â–∏–π—Å—è –≤ —Ç–∞–±–ª–∏—Ü–µ
                    pass

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ç–∞–±–ª–∏—Ü
            if isinstance(element, LTRect):
                # –ï—Å–ª–∏ –ø–µ—Ä–≤—ã–π –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç
                if first_element == True and (table_num + 1) <= len(tables):
                    # –ù–∞—Ö–æ–¥–∏–º –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏–π –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫ —Ç–∞–±–ª–∏—Ü—ã
                    lower_side = page.bbox[3] - tables[table_num].bbox[3]
                    upper_side = element.y1
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ç–∞–±–ª–∏—Ü—ã
                    table = extract_table(pdf_path, pagenum, table_num)
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Ç–∞–±–ª–∏—Ü—ã –≤ —Ñ–æ—Ä–º–∞—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
                    table_string = table_converter(table)
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É —Ç–∞–±–ª–∏—Ü—ã –≤ —Å–ø–∏—Å–æ–∫
                    text_from_tables.append(table_string)
                    page_content.append(table_string)
                    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ True, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
                    table_extraction_flag = True
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –¥—Ä—É–≥–æ–π —ç–ª–µ–º–µ–Ω—Ç
                    first_element = False
                    # –î–æ–±–∞–≤–ª—è–µ–º —É—Å–ª–æ–≤–Ω–æ–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Å–ø–∏—Å–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∞
                    page_text.append('table')
                    line_format.append('table')

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–≤–ª–µ–∫–ª–∏ –ª–∏ –º—ã —É–∂–µ —Ç–∞–±–ª–∏—Ü—ã –∏–∑ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                if element.y0 >= lower_side and element.y1 <= upper_side:
                    pass
                elif not isinstance(page_elements[i + 1][1], LTRect):
                    table_extraction_flag = False
                    first_element = True
                    table_num += 1

        # –°–æ–∑–¥–∞—ë–º –∫–ª—é—á –¥–ª—è —Å–ª–æ–≤–∞—Ä—è
        dctkey = 'Page_' + str(pagenum)
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –∫–∞–∫ –∑–Ω–∞—á–µ–Ω–∏–µ –∫–ª—é—á–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        text_per_page[dctkey] = [page_text, line_format, text_from_images, text_from_tables, page_content]

    # –ó–∞–∫—Ä—ã–≤–∞–µ–º –æ–±—ä–µ–∫—Ç —Ñ–∞–π–ª–∞ pdf
    pdfFileObj.close()

    # –£–¥–∞–ª—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    result = ''.join(text_per_page['Page_0'][4])
    print(result)
    text = result.replace("\n", " ")
    keyphrases = extractor(text)
    print(keyphrases)
    else:
        text = filereader(file_path).replace("\n", " ")
        keyphrases = extractor(text)
        print(keyphrases)