
import streamlit as st
import easyocr
import json
import bs4
import requests


# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
page_title="–ó–¥–µ—Å—å –±—É–¥–µ—Ç –∫—Ä–∞—Å–∏–≤–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ",
page_icon="üßä")

# –ò–∑–º–µ–Ω–µ–Ω–∏–µ –¥–∏–∑–∞–π–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
page_element="""
<style>
[data-testid="stAppViewContainer"]{
  background-image: url("https://i.artfile.ru/1920x1080_927461_[www.ArtFile.ru].jpg");
  background-size: cover;
}
[data-testid="baseButton-secondary"]{
background-color: rgb(176 96 255 / 70%);
}
[role="tablist"]{
background-color: rgb(210 164 255 / 90%);
border-radius: 10px;
}
[data-testid="stHeader"]{
  background-color: rgba(0,0,0,0);
}
.st-b1{
  margin: auto;
}
[data-testid="block-container"]
 { background-color: rgba(255, 255, 255, 0.4)
}
.st-cc {
    margin: auto;
}
</style>
"""

st.markdown(page_element, unsafe_allow_html=True)

# –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.header('–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É–µ–º! –ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.')

# –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –≤–∫–ª–∞–¥–æ–∫ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞
tabs = st.tabs(["–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ", "–ü–æ–∏—Å–∫"])

# –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –≤–∫–ª–∞–¥–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
with tabs[0]:
    @st.cache_resource()
    def easyocr_recognition(img):
        return easyocr.Reader(["ru", "en", "uk"]).readtext(img, detail=0, paragraph=True, text_threshold=0.8)

    # —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é easyocr, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: –æ—Ç–∫–ª—é—á–µ–Ω–∞ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã–≤–æ–¥–∞,
    # –≤–∫–ª—é—á–µ–Ω—ã –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞

    # —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª

    def load_image():
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–æ—Ä–º—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        # –§–æ—Ä–º–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏ Streamlit
        uploaded_file = st.file_uploader(
            label='–í—ã–±–µ—Ä–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, —Å–æ–¥–µ—Ä–∂–∞—â–µ–µ —Ç–µ–∫—Å—Ç, –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è.\n –§–æ—Ä–º–∞—Ç –∑–∞–≥—Ä—É–∂–∞–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞ .jpg .png')
        if uploaded_file is not None:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            img = uploaded_file.getvalue()
            # –ü–æ–∫–∞–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ Web-—Å—Ç—Ä–∞–Ω–∏—Ü–µ —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏ Streamlit
            st.image(img)
            return img
        else:
            return st.write('–í—ã –Ω–∏—á–µ–≥–æ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏!')

    # –í—ã–≤–æ–¥–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏ Streamlit
    st.header('–≠—Ç–æ –ø—Ä–æ—Å—Ç–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö —Ç–µ–∫—Å—Ç')

    # –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–æ—Ä–º—ã –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

    img = load_image()

    result = st.button('–†–∞—Å–ø–æ–∑–Ω–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ —Ñ–∞–π–ª–∞')
    if result:
        try:
            text_result = easyocr_recognition(img)
            st.write(text_result)
        except:
            st.write('–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ñ–∞–π–ª –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏–ª–∏ –Ω–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞!')

# –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –≤–∫–ª–∞–¥–∫–∏ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
with tabs[1]:
    st.header('–≠—Ç–æ –ø—Ä–æ—Å—Ç–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º')
# —Ç–µ–ª–æ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    class Article:

        def __init__(self, title='', link='', authors=''):
            self.__title = title
            self.__link = link
            self.__authors = authors
            self.__year = 0
            self.__rsci = False
            self.__vak = False
            self.__scopus = False

        def check_filter(self, filter):
            if filter == 22 and not self.__rsci or filter == 8 and not self.__vak or filter == 2 and not self.__scopus:
                return False
            return True

        @property
        def title(self):
            return self.__title

        @title.setter
        def title(self, value):
            self.__title = value

        @property
        def link(self):
            return self.__link

        @link.setter
        def link(self, value):
            self.__link = value

        @property
        def authors(self):
            return self.__authors

        @authors.setter
        def authors(self, value):
            self.__authors = value

        @property
        def year(self):
            return self.__year

        @year.setter
        def year(self, value):
            self.__year = value

        @property
        def rsci(self):
            return self.__rsci

        @rsci.setter
        def rsci(self, value):
            self.__rsci = value

        @property
        def vak(self):
            return self.__vak

        @vak.setter
        def vak(self, value):
            self.__vak = value

        @property
        def scopus(self):
            return self.__scopus

        @scopus.setter
        def scopus(self, value):
            self.__scopus = value

    TERM_LINK = '"link":'
    TERM_FOUND = '"found":'

    API = 'https://cyberleninka.ru/api/search'
    URL = 'https://cyberleninka.ru'

    REQUEST_BODY = {
        'mode': 'articles',
    '   size': 10,
    }

    ARTICLES_PER_PAGE = 10


    class Searcher:

        def __try_parse_article(self, link, article):
            response_page = requests.get(link)
            bs_page = bs4.BeautifulSoup(response_page.text, 'html.parser')
            try:
                article.title = bs_page.i.text
            except:
                print('done')

            authors = bs_page.find('h2', {'class': 'right-title'}).span.text
            article.authors = authors[authors.find('‚Äî') + 2:]

            labels = bs_page.find('div', {'class': 'labels'})
            article.year = int(labels.time.text)

            rsci = bs_page.find('div', {'class': 'label rsci'})
            if rsci:
                article.rsci = True

            vak = bs_page.find('div', {'class': 'label vak'})
            if vak:
                article.vak = True

            scopus = bs_page.find('div', {'class': 'label scopus'})
            if scopus:
                article.scopus = True

        def parse_article_page(self, link):
            article = Article(link=link)

            try:
                self.__try_parse_article(link, article)

            except requests.HTTPError or AttributeError as e:
                print(e)

            return article

        def __try_parse_request(self, body, filters, results, articles_per_page=ARTICLES_PER_PAGE):
            response_json = requests.post(API, data=json.dumps(body)).text

            for article_num in range(articles_per_page):
                response_json = response_json[response_json.find(TERM_LINK) + len(TERM_LINK) + 1:]
                article_link = response_json[:response_json.find('"')]
                article = self.parse_article_page(URL + article_link)
                if filters is None or article.check_filter(filters[0]):
                    results.append(article)
                response_json = response_json[response_json.find('}'):]

        def search_articles(self, keywords, max_page, filters=None):
            results = []
            found = 0
            body = {
                'mode': 'articles',
                'size': 10,
                'q': keywords}

            if filters:
                body = body | {'catalogs': filters}

            body['from'] = 0

            try:
                response_json = requests.post(API, data=json.dumps(body)).text
                response_found = response_json[response_json.find(TERM_FOUND) + len(TERM_FOUND):]
                #print(response_found)
                #print(response_json)
                resp_data = json.loads(response_json)
                relevant = resp_data["articles"]
                for i in relevant:

                    def special_char_fix(string):
                        string = list(string)
                        for pl, char in enumerate(string):
                            if char == '\\':
                                val = ''.join([string[pl + k + 2] for k in range(4)])
                                for k in range(5):
                                    string.pop(pl)
                                string[pl] = str(chr(int(val, 16)))
                        return ''.join(string)

                    st.write("–ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏:" + ' ' + (special_char_fix(i["name"])) + ' ' + '–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è:' + ' ' + i["annotation"] + ' ' + i["link"], end='\n')
                found = int(response_found[:response_found.find(',')])

            except requests.HTTPError or AttributeError as e:
                print(e)

            for page in range(max_page):
                try:
                    if found >= ARTICLES_PER_PAGE:
                        body['from'] += 10 * page
                        self.__try_parse_request(body, filters, results)
                        found -= ARTICLES_PER_PAGE
                    else:
                        body['from'] += 10 * (page - 1)
                        self.__try_parse_request(body, filters, results, found)
                        return results

                except requests.HTTPError as e:
                    print(e)
                    return []

            return results




#if __name__ == '__main__':
    keywords = st.text_input(('Input your keywords.\n').lower())
    pr = Searcher()
    pr.search_articles(keywords, 10)

    #st.write('–†–∞–∑–¥–µ–ª –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ!')
    #st.image('http://www.opsa.info/img/innovations/v-nastoyaschee-vremya-razdel-nahoditsya-v-razrabotke.jpg')
